{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c35940c",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition with Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67931c99",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83758c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from IPython.display import SVG, Image\n",
    "from livelossplot import PlotLossesKeras\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746dfdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995 angry images\n",
      "436 disgust images\n",
      "4097 fear images\n",
      "7215 happy images\n",
      "4965 neutral images\n",
      "4830 sad images\n",
      "3171 surprise images\n"
     ]
    }
   ],
   "source": [
    "for expression in os.listdir(\"train/\"):\n",
    "    print(str(len(os.listdir(\"train/\" + expression))) + \" \" + expression + \" images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06a311",
   "metadata": {},
   "source": [
    "# Generate Training and Validation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7856f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "img_size = 48\n",
    "batch_size = 64\n",
    "\n",
    "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "train_generator = datagen_train.flow_from_directory(\"train/\",\n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
    "validation_generator = datagen_validation.flow_from_directory(\"test/\",\n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)\n",
    "\n",
    "datagen_test = ImageDataGenerator()\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "        \"C:/Users/megha/test/\",\n",
    "        target_size=(48, 48),\n",
    "        color_mode=\"grayscale\",\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b289f8d",
   "metadata": {},
   "source": [
    "# Creating CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276c7cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 48, 48, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 512)       590336    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 12, 12, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 12, 12, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 6, 6, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,478,727\n",
      "Trainable params: 4,474,759\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1ee15",
   "metadata": {},
   "source": [
    "# Training and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e6b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.7820 - accuracy: 0.3163\n",
      "Epoch 1: saving model to model_weights.h5\n",
      "448/448 [==============================] - 398s 886ms/step - loss: 1.7820 - accuracy: 0.3163 - val_loss: 2.0685 - val_accuracy: 0.3333 - lr: 5.0000e-04\n",
      "Epoch 2/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4680 - accuracy: 0.4358\n",
      "Epoch 2: saving model to model_weights.h5\n",
      "448/448 [==============================] - 280s 625ms/step - loss: 1.4680 - accuracy: 0.4358 - val_loss: 1.3264 - val_accuracy: 0.4907 - lr: 5.0000e-04\n",
      "Epoch 3/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3203 - accuracy: 0.4943\n",
      "Epoch 3: saving model to model_weights.h5\n",
      "448/448 [==============================] - 285s 636ms/step - loss: 1.3203 - accuracy: 0.4943 - val_loss: 1.3137 - val_accuracy: 0.4814 - lr: 5.0000e-04\n",
      "Epoch 4/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.2403 - accuracy: 0.5278\n",
      "Epoch 4: saving model to model_weights.h5\n",
      "448/448 [==============================] - 293s 653ms/step - loss: 1.2403 - accuracy: 0.5278 - val_loss: 1.2562 - val_accuracy: 0.5134 - lr: 5.0000e-04\n",
      "Epoch 5/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.1904 - accuracy: 0.5438\n",
      "Epoch 5: saving model to model_weights.h5\n",
      "448/448 [==============================] - 297s 664ms/step - loss: 1.1904 - accuracy: 0.5438 - val_loss: 1.1777 - val_accuracy: 0.5427 - lr: 5.0000e-04\n",
      "Epoch 6/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.1459 - accuracy: 0.5643\n",
      "Epoch 6: saving model to model_weights.h5\n",
      "448/448 [==============================] - 298s 665ms/step - loss: 1.1459 - accuracy: 0.5643 - val_loss: 1.3073 - val_accuracy: 0.5146 - lr: 5.0000e-04\n",
      "Epoch 7/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.1177 - accuracy: 0.5766\n",
      "Epoch 7: saving model to model_weights.h5\n",
      "448/448 [==============================] - 293s 653ms/step - loss: 1.1177 - accuracy: 0.5766 - val_loss: 1.1460 - val_accuracy: 0.5714 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.0848 - accuracy: 0.5872\n",
      "Epoch 8: saving model to model_weights.h5\n",
      "448/448 [==============================] - 289s 645ms/step - loss: 1.0848 - accuracy: 0.5872 - val_loss: 1.2566 - val_accuracy: 0.5188 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.0614 - accuracy: 0.5980\n",
      "Epoch 9: saving model to model_weights.h5\n",
      "448/448 [==============================] - 285s 636ms/step - loss: 1.0614 - accuracy: 0.5980 - val_loss: 1.1796 - val_accuracy: 0.5340 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9893 - accuracy: 0.6256\n",
      "Epoch 10: saving model to model_weights.h5\n",
      "448/448 [==============================] - 278s 621ms/step - loss: 0.9893 - accuracy: 0.6256 - val_loss: 1.0009 - val_accuracy: 0.6235 - lr: 5.0000e-05\n",
      "Epoch 11/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9706 - accuracy: 0.6348\n",
      "Epoch 11: saving model to model_weights.h5\n",
      "448/448 [==============================] - 824s 2s/step - loss: 0.9706 - accuracy: 0.6348 - val_loss: 0.9949 - val_accuracy: 0.6296 - lr: 5.0000e-05\n",
      "Epoch 12/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9585 - accuracy: 0.6389\n",
      "Epoch 12: saving model to model_weights.h5\n",
      "448/448 [==============================] - 275s 615ms/step - loss: 0.9585 - accuracy: 0.6389 - val_loss: 0.9883 - val_accuracy: 0.6316 - lr: 5.0000e-05\n",
      "Epoch 13/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 0.6461\n",
      "Epoch 13: saving model to model_weights.h5\n",
      "448/448 [==============================] - 279s 624ms/step - loss: 0.9444 - accuracy: 0.6461 - val_loss: 0.9868 - val_accuracy: 0.6328 - lr: 5.0000e-05\n",
      "Epoch 14/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.6493\n",
      "Epoch 14: saving model to model_weights.h5\n",
      "448/448 [==============================] - 280s 626ms/step - loss: 0.9328 - accuracy: 0.6493 - val_loss: 0.9952 - val_accuracy: 0.6303 - lr: 5.0000e-05\n",
      "Epoch 15/15\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.6511\n",
      "Epoch 15: saving model to model_weights.h5\n",
      "448/448 [==============================] - 276s 616ms/step - loss: 0.9264 - accuracy: 0.6511 - val_loss: 0.9905 - val_accuracy: 0.6343 - lr: 5.0000e-05\n",
      "Wall time: 1h 22min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 15\n",
    "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
    "validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=0.00001, mode='auto')\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n",
    "                             save_weights_only=True, mode='max', verbose=1)\n",
    "callbacks = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe865280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eedfedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "fear\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=48x48 at 0x1A07A411CA0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the images in the test set\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "predict = np.argmax(model.predict(test_generator,steps = nb_samples),axis=1)\n",
    "\n",
    "# load all images into a list\n",
    "folder_path = 'C:/Users/megha/test/'\n",
    "img_width=48\n",
    "img_height=48\n",
    "images = []\n",
    "for img in filenames:\n",
    "    img = os.path.join(folder_path, img)\n",
    "    img = image.load_img(img, color_mode = \"grayscale\",target_size=(img_width, img_height))\n",
    "    images.append(img)\n",
    "\n",
    "# predicting a random image from test set \n",
    "n = random.randint(0,len(predict))\n",
    "print(predict[n])\n",
    "print(list(train_generator.class_indices.keys())[list(train_generator.class_indices.values()).index(predict[n])])\n",
    "images[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4a94a",
   "metadata": {},
   "source": [
    "# Representing Model as JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d8d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc05ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
